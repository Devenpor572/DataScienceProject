{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_imdb():\n",
    "    import csv\n",
    "    import re\n",
    "    from requests import get\n",
    "    from bs4 import BeautifulSoup\n",
    "    from IPython.core.display import clear_output\n",
    "    from warnings import warn\n",
    "    from time import time, sleep\n",
    "    from random import gauss, shuffle, randint\n",
    "\n",
    "\n",
    "    start_time = time()\n",
    "    html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    page_count = 10\n",
    "\n",
    "    years = [i for i in range(1935, 2020)]\n",
    "    year_pages = [(year, page) for year in years for page in range(page_count)]\n",
    "    # shuffle(year_pages)\n",
    "\n",
    "    headers = {\"Accept-Language\": \"en-US, en;q=0.5\"}\n",
    "\n",
    "    # Preparing the monitoring of the loop\n",
    "    start_time = time()\n",
    "    requests = 0\n",
    "\n",
    "    # Write the header to file before looping\n",
    "    with open('output/movie_ratings.csv', 'w', newline='') as output_file:\n",
    "        csv_headers = ['imdb_ids', 'movie', 'year', 'imdb', 'metascore', 'votes']\n",
    "        writer = csv.DictWriter(output_file, fieldnames=csv_headers)\n",
    "        writer.writeheader()\n",
    "\n",
    "    for year, page in year_pages:\n",
    "\n",
    "        # Redeclaring the lists to store data in\n",
    "        names = []\n",
    "        years = []\n",
    "        imdb_ratings = []\n",
    "        metascores = []\n",
    "        votes = []\n",
    "        imdb_ids = []\n",
    "\n",
    "        # Make a get request\n",
    "        start = str((page * 50) + 1)\n",
    "        url = 'http://www.imdb.com/search/title?release_date=' + str(year) + '&sort=num_votes,desc&start=' + start \n",
    "        while(requests / elapsed_time > 0.1):\n",
    "            sleep(randint(1, 10))\n",
    "            elapsed_time = time() - start_time\n",
    "        response = get(url, headers = headers)\n",
    "\n",
    "        # Pause the loop\n",
    "        sleep_time = gauss(10, 4)\n",
    "        sleep(abs(sleep_time))\n",
    "\n",
    "        # Monitor the requests\n",
    "        requests += 1\n",
    "        elapsed_time = time() - start_time\n",
    "        print('Request:{}; Frequency: {} requests/s\\nLast URL: {}'.format(requests, requests/elapsed_time, url))\n",
    "        clear_output(wait = True)\n",
    "\n",
    "        # Throw a warning for non-200 status codes\n",
    "        if response.status_code != 200:\n",
    "            warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "            continue\n",
    "\n",
    "        # Break the loop if the number of requests is greater than expected\n",
    "        if requests > 2000:\n",
    "            warn('Number of requests was greater than expected.')\n",
    "            break\n",
    "\n",
    "        # Parse the content of the request with BeautifulSoup\n",
    "        page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Select all the 50 movie containers from a single page\n",
    "        mv_containers = page_html.find_all('div', class_ = 'lister-item mode-advanced')\n",
    "\n",
    "        # For every movie of these 50\n",
    "        for container in mv_containers:\n",
    "            # Scrape the name\n",
    "            title_html = container.h3.a\n",
    "            imdb_id = re.search('/title/tt(\\d+)/', container.h3.a['href']).group(1)\n",
    "            imdb_ids.append(imdb_id)\n",
    "            names.append(title_html.text)\n",
    "\n",
    "            # Scrape the year\n",
    "            # year = container.h3.find('span', class_ = 'lister-item-year').text\n",
    "            years.append(year)\n",
    "\n",
    "            # Scrape the IMDB rating\n",
    "            imdb = float(container.strong.text)\n",
    "            imdb_ratings.append(imdb)\n",
    "\n",
    "            # Scrape the Metascore\n",
    "            if container.find('div', class_ = 'ratings-metascore') is not None:\n",
    "                m_score = container.find('span', class_ = 'metascore').text\n",
    "                metascores.append(int(m_score))\n",
    "            else:\n",
    "                metascores.append(None)\n",
    "\n",
    "            # Scrape the number of votes\n",
    "            vote = container.find('span', attrs = {'name':'nv'})['data-value']\n",
    "            votes.append(int(vote))\n",
    "\n",
    "        movie_ratings_df = pd.DataFrame({'imdb_ids': imdb_ids,\n",
    "                                         'movie': names,\n",
    "                                         'year': years,\n",
    "                                         'imdb': imdb_ratings,\n",
    "                                         'metascore': metascores,\n",
    "                                         'votes': votes})\n",
    "        # Checkpoint data as you go\n",
    "        with open('output/movie_ratings.csv', 'a', newline='') as output_file:\n",
    "            movie_ratings_df.to_csv(output_file, header=False, index=False)\n",
    "\n",
    "    sleep(1)\n",
    "    clear_output(wait = True)\n",
    "\n",
    "    normalized_movie_ratings_df = pd.read_csv('output/movie_ratings.csv', encoding='ISO-8859-1')\n",
    "    normalized_movie_ratings_df = normalized_movie_ratings_df[['imdb_ids', 'movie', 'year', 'imdb', 'metascore', 'votes']]\n",
    "    normalized_movie_ratings_df['imdb'] = normalized_movie_ratings_df['imdb'] * 10\n",
    "    normalized_movie_ratings_df['imdb'] = normalized_movie_ratings_df['imdb'].astype(int)\n",
    "    normalized_movie_ratings_df.to_csv('output/norm_movie_ratings.csv')\n",
    "\n",
    "    display(normalized_movie_ratings_df)\n",
    "scrape_imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def scrape_thenumbers():\n",
    "    import bs4\n",
    "    import requests\n",
    "    import json\n",
    "    import os\n",
    "    import re\n",
    "    import time\n",
    "    # There are 5860 movies at the time this program was written\n",
    "    PAGE_COUNT = 59\n",
    "\n",
    "\n",
    "    class SimpleLogger(object):\n",
    "        def __init__(self, filename):\n",
    "            self.file = open(filename, 'w')\n",
    "\n",
    "        def __del__(self):\n",
    "            self.file.close()\n",
    "\n",
    "        def log(self, msg):\n",
    "            print(msg)\n",
    "            print(msg, file=self.file)\n",
    "\n",
    "    def parse_dollars(dollar_str):\n",
    "        return int(dollar_str.replace('$', '').replace(',', ''))\n",
    "\n",
    "\n",
    "    def clean_input(input_str):\n",
    "        return input_str.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "\n",
    "\n",
    "    def scrape_movie(url_ext):\n",
    "        page_str = 'https://www.the-numbers.com' + url_ext\n",
    "        movie_rsp = requests.get(page_str)\n",
    "        \n",
    "        try:\n",
    "            movie_rsp.raise_for_status()\n",
    "        except requests.HTTPError:\n",
    "            logger.log('Request for ' + page_str + ' failed!')\n",
    "            return None\n",
    "        parsed_movie_rsp = bs4.BeautifulSoup(clean_input(movie_rsp.text), features='html.parser')\n",
    "        table = parsed_movie_rsp.find(text='Movie Details').parent.find_next('table')\n",
    "        mpaa_rating = None\n",
    "        running_time = None\n",
    "        franchises = None\n",
    "        keywords = None\n",
    "        source = None\n",
    "        genre = None\n",
    "        production_method = None\n",
    "        creative_type = None\n",
    "        production_companies = None\n",
    "        production_countries = None\n",
    "        languages = None\n",
    "        temp = table.find(text='MPAA Rating:')\n",
    "        if temp:\n",
    "            result = re.search('/market/mpaa-rating/([\\w\\d\\(\\)-]+)', temp.parent.parent.parent.select('a')[0]['href'])\n",
    "            mpaa_rating = {\n",
    "                'slug': result.group(1) if result else None,\n",
    "                'pretty': temp.parent.parent.parent.select('a')[0].text\n",
    "            }\n",
    "        temp = table.find(text='Running Time:')\n",
    "        if temp:\n",
    "            temp = temp.parent.parent.find_next('td').getText()\n",
    "            running_time = int(temp.split(' ')[0])\n",
    "        temp = table.find(text='Franchise:')\n",
    "        if temp:\n",
    "            franchises = list()\n",
    "            for franchise in temp.parent.find_next('td').find_all('a'):\n",
    "                result = re.search('/movies/franchise/([\\w\\d\\(\\)-]+)', franchise['href'])\n",
    "                franchises.append({\n",
    "                    'slug': result.group(1) if result else None,\n",
    "                    'pretty': franchise.text\n",
    "                })\n",
    "        temp = table.find(text='Keywords:')\n",
    "        if temp:\n",
    "            keywords = list()\n",
    "            for keyword in temp.parent.find_next('td').find_all('a'):\n",
    "                result = re.search('/movies/keywords/([\\w\\d\\(\\)-]+)', keyword['href'])\n",
    "                keywords.append({\n",
    "                    'slug': result.group(1) if result else None,\n",
    "                    'pretty': keyword.text\n",
    "                })\n",
    "        temp = table.find(text='Source:')\n",
    "        if temp:\n",
    "            result = re.search('/market/source/([\\w\\d\\(\\)-]+)', temp.parent.parent.parent.select('a')[0]['href'])\n",
    "            source = {\n",
    "                'slug': result.group(1) if result else None,\n",
    "                'pretty': temp.parent.parent.parent.select('a')[0].getText()\n",
    "            }\n",
    "        temp = table.find(text='Genre:')\n",
    "        if temp:\n",
    "            result = re.search('/market/genre/([\\w\\d\\(\\)-]+)', temp.parent.parent.parent.select('a')[0]['href'])\n",
    "            genre = {\n",
    "                'slug': result.group(1) if result else None,\n",
    "                'pretty': temp.parent.parent.parent.select('a')[0].getText()\n",
    "            }\n",
    "        temp = table.find(text='Production Method:')\n",
    "        if temp:\n",
    "            result = re.search('/market/production-method/([\\w\\d\\(\\)-]+)', temp.parent.parent.parent.select('a')[0]['href']) \n",
    "            production_method = {\n",
    "                'slug': result.group(1) if result else None,\n",
    "                'pretty': temp.parent.parent.parent.select('a')[0].getText()\n",
    "            }\n",
    "        temp = table.find(text='Creative Type:')\n",
    "        \n",
    "        print(table)\n",
    "        print(temp)\n",
    "        if temp:\n",
    "            result = re.search('/market/creative-type/([\\w\\d\\(\\)-]+)', temp.parent.parent.parent.select('a')[0]['href'])\n",
    "            creative_type = {\n",
    "                'slug': result.group(1) if result else None,\n",
    "                'pretty': temp.parent.parent.parent.select('a')[0].getText()\n",
    "            }\n",
    "        temp = table.find(text='Production Companies:')\n",
    "        if temp:\n",
    "            production_companies = list()\n",
    "            for production_company in temp.parent.find_next('td').find_all('a'):\n",
    "                result = re.search('/movies/production-company/([\\w\\d\\(\\)-]+)', production_company['href'])\n",
    "                production_companies.append({\n",
    "                    'slug': result.group(1) if result else None,\n",
    "                    'pretty': production_company.text\n",
    "                })\n",
    "        temp = table.find(text='Production Countries:')\n",
    "        if temp:\n",
    "            production_countries = list()\n",
    "            for production_country in temp.parent.find_next('td').find_all('a'):\n",
    "                result = re.search('/([\\w\\d\\(\\)-]+)/movies', production_country['href'])\n",
    "                production_countries.append({\n",
    "                    'slug': result.group(1) if result else None, \n",
    "                    'pretty': production_country.text\n",
    "                })\n",
    "        temp = table.find(text='Languages:')\n",
    "        if temp:\n",
    "            languages = list()\n",
    "            for language in temp.parent.find_next('td').find_all('a'):\n",
    "                result = re.search('/language/([\\w\\d\\(\\)-]+)/movies', language['href'])\n",
    "                languages.append({\n",
    "                    'slug': result.group(1) if result else None,\n",
    "                    'pretty': language.text\n",
    "                })\n",
    "        movie_dict = {\n",
    "            'mpaa-rating': mpaa_rating,\n",
    "            'running-time': running_time,\n",
    "            'franchises': franchises,\n",
    "            'keywords': keywords,\n",
    "            'source': source,\n",
    "            'genre': genre,\n",
    "            'production_method': production_method,\n",
    "            'creative_type': creative_type,\n",
    "            'production_companies': production_companies,\n",
    "            'production_countries': production_countries,\n",
    "            'languages': languages\n",
    "        }\n",
    "        return movie_dict['creative_type']\n",
    "\n",
    "    print(scrape_movie('/movie/Star-Wars-Ep-VIII-The-Last-Jedi'))\n",
    "    return\n",
    "\n",
    "    def scrape_list_page(url_ext):\n",
    "        count = 0\n",
    "        page_str = 'https://www.the-numbers.com/movie/budgets/all' + url_ext\n",
    "        movie_lst_rsp = requests.get(page_str)\n",
    "        # Halt if there was an issue with the request\n",
    "        try:\n",
    "            movie_lst_rsp.raise_for_status()\n",
    "        except requests.HTTPError:\n",
    "            logger.log('Request for ' + page_str + ' failed!')\n",
    "            return None\n",
    "        else:\n",
    "            logger.log('Request for ' + page_str + ' succeeded')\n",
    "        parsed_movie_lst_rsp = bs4.BeautifulSoup(clean_input(movie_lst_rsp.text), features=\"html.parser\")\n",
    "        for table_row in parsed_movie_lst_rsp.find_all('tr')[1:]:\n",
    "            table_cells = table_row.find_all('td')\n",
    "            i = 0\n",
    "            movie_dict = dict()\n",
    "            result = re.search('/box-office-chart/daily/(\\d{4}/\\d{1,2}/\\d{1,2})', table_cells[i + 1].a['href'])\n",
    "            movie_dict['release_date'] = {\n",
    "                'slug': result.group(1) if result else None,\n",
    "                'pretty': table_cells[i + 1].a.text\n",
    "            }\n",
    "            movie_dict['title'] = {\n",
    "                'slug': re.search('/movie/([\\w\\d\\(\\)-]+)#tab=summary', table_cells[i + 2].b.a['href']).group(1),\n",
    "                'pretty': table_cells[i + 2].b.a.text\n",
    "            }\n",
    "            # Scrape the movie page\n",
    "            movie_dict['production_budget'] = parse_dollars(table_cells[i + 3].getText())\n",
    "            movie_dict['domestic_gross'] = parse_dollars(table_cells[i + 4].getText())\n",
    "            movie_dict['worldwide_gross'] = parse_dollars(table_cells[i + 5].getText())\n",
    "            movie_dict.update(scrape_movie(table_cells[i + 2].select('a')[0]['href']))\n",
    "            yield movie_dict\n",
    "            count += 1\n",
    "        logger.log('Read ' + str(count) + ' entries from page ' + page_str)\n",
    "\n",
    "\n",
    "    def scrape_the_numbers_generator():\n",
    "        url_ext_lst = [''] + ['/' + str(x) + '01' for x in range(1, PAGE_COUNT)]\n",
    "        for url_ext in url_ext_lst:\n",
    "            for page in scrape_list_page(url_ext):\n",
    "                if page is None:\n",
    "                    logger.log('Page {} returned None'.format(url_ext))\n",
    "                    break\n",
    "                else:\n",
    "                    yield page\n",
    "\n",
    "\n",
    "    def store_the_numbers(filename):\n",
    "        start_total = time.time()\n",
    "        i = 1\n",
    "        start_row = time.time()\n",
    "        entries = list()\n",
    "        for entry in scrape_the_numbers_generator():\n",
    "            entries.append(entry)\n",
    "            exec_time_row = time.time() - start_row\n",
    "            logger.log('In {:.3f} seconds - {:0>4} {}'.format(exec_time_row, i, entry['title']['pretty']))\n",
    "            start_row = time.time()\n",
    "            i += 1\n",
    "            if i % 100 == 0:\n",
    "                with open(filename, 'w', newline='') as outfile:\n",
    "                    json.dump(entries, outfile)\n",
    "        with open(filename, 'w', newline='') as outfile:\n",
    "                    json.dump(entries, outfile)\n",
    "        exec_time_total = time.time() - start_total\n",
    "        logger.log('{} total entries in {:.3f} seconds'.format(i - 1, exec_time_total))\n",
    "\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        filename = 'output/the_numbers.json'        \n",
    "        # We really don't want to overwrite any existing good DB file, it takes a long time to scrape all of the data\n",
    "        if os.path.isfile(filename):\n",
    "            exit()\n",
    "        logger = SimpleLogger('log.txt')\n",
    "        store_the_numbers(filename)\n",
    "\n",
    "scrape_thenumbers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_movies():\n",
    "    import json\n",
    "    import re\n",
    "    import time\n",
    "    import tmdbsimple as tmdb\n",
    "    from requests import HTTPError\n",
    "    import sys\n",
    "    \n",
    "    tmdb.API_KEY = 'af3f6fb737c9085bd424c403dc83c196'\n",
    "    \n",
    "    def simple_test():\n",
    "        search = tmdb.Search()\n",
    "        results = search.movie(**{'query': 'Star Wars Episode VIII The Last Jedi', \n",
    "                                  'year': '2017'})\n",
    "        movie = tmdb.Movies(results['results'][0]['id'])\n",
    "        results = movie.external_ids()\n",
    "        print(movie.external_ids()['imdb_id'])\n",
    "        sys.exit()\n",
    "    # simple_test()\n",
    "    \n",
    "    class SimpleLogger(object):\n",
    "        def __init__(self, filename):\n",
    "            self.file = open(filename, 'w')\n",
    "\n",
    "        def __del__(self):\n",
    "            self.file.close()\n",
    "\n",
    "        def log(self, msg):\n",
    "            print(msg)\n",
    "            print(msg, file=self.file)\n",
    "            \n",
    "    def clean_string(in_str):\n",
    "        return re.sub('[^A-Za-z0-9 \\-]+', '', in_str)\n",
    "    \n",
    "    def normalize_movie_title(in_str):\n",
    "        in_str = clean_string(in_str)\n",
    "        in_str = re.sub(' ([Ee]p) ', ' \\g<1>isode ', in_str)\n",
    "        return in_str\n",
    "            \n",
    "    def search_imdb_id(title, year):\n",
    "        # print('\\\"{}\\\" ({})'.format(title, year))\n",
    "        search = tmdb.Search()\n",
    "        query_dict = {'query': title}\n",
    "        if year is not None:\n",
    "            query_dict['year'] = str(year)\n",
    "        results = search.movie(**query_dict)\n",
    "        if (not 'results' in results) or (not results['results']) or (not 'id' in results['results'][0]):\n",
    "            return None\n",
    "        movie = tmdb.Movies(results['results'][0]['id'])\n",
    "        results = movie.external_ids()\n",
    "        if not 'imdb_id' in results or results['imdb_id'] is  None:\n",
    "            return None\n",
    "        result = re.search('tt(\\d+)', results['imdb_id'])\n",
    "        return result.group(1) if result else None\n",
    "    \n",
    "    \n",
    "    def normalize_the_numbers():\n",
    "        with open('data/the_numbers/the_numbers.json', 'r') as input_file:\n",
    "            the_numbers = json.load(input_file)\n",
    "        for i, movie in enumerate(the_numbers):\n",
    "            year = None\n",
    "            if movie['release_date']['slug'] is not None:\n",
    "                result = re.search('^(\\d{4}).*', movie['release_date']['slug'])\n",
    "                year = int(result.group(1)) if result is not None else None\n",
    "                if 1900 < year < 2030:\n",
    "                    year = str(year)\n",
    "                else:\n",
    "                    year = None\n",
    "            try:\n",
    "                imdb_id = search_imdb_id(normalize_movie_title(movie['title']['slug']), year)\n",
    "                if imdb_id is None:\n",
    "                    time.sleep(0.26)\n",
    "                    imdb_id = search_imdb_id(normalize_movie_title(movie['title']['pretty']), year)\n",
    "                the_numbers[i]['imdb_id'] = imdb_id\n",
    "            except HTTPError:\n",
    "                print('HTTPError: {} ({})'.format(title, year))\n",
    "            # API Limited to 40 calls every 10 seconds, search_imdb_id calls API twice for each movie\n",
    "            logger.log('{:0>4} tt{: <10} {} ({})'.format(i, imdb_id if imdb_id is not None else '', movie['title']['pretty'], year))\n",
    "            time.sleep(0.51)\n",
    "        with open('output/the_numbers_norm.json', 'w', newline='') as outfile:\n",
    "            json.dump(the_numbers, outfile)\n",
    "    \n",
    "    logger = SimpleLogger('normalize_log.txt')\n",
    "    normalize_the_numbers()\n",
    "\n",
    "normalize_movies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectify_the_numbers():\n",
    "    import csv\n",
    "    import json\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    \n",
    "    def rectify_single(the_numbers, output_dir):\n",
    "        single_columns = \\\n",
    "        ['release_date', 'release_date_norm', 'title', 'title_norm', 'mpaa-rating', \n",
    "         'mpaa-rating_norm', 'running-time', 'source', 'source_norm', 'genre', \n",
    "         'genre_norm', 'production_method', 'production_method_norm', \n",
    "         'creative_type', 'creative_type_norm', 'production_budget', 'domestic_gross', \n",
    "         'worldwide_gross', 'imdb_id']\n",
    "        \n",
    "        with open(os.path.join(output_dir, 'the_numbers_rectified.csv'), 'w', newline='') as outfile:\n",
    "            header_writer = csv.writer(outfile)\n",
    "            header_writer.writerow(columns)\n",
    "        i = 0\n",
    "        for entry in the_numbers:\n",
    "            rectified_entry = dict()\n",
    "            for key, value in entry.items():\n",
    "                if key is None:\n",
    "                    continue\n",
    "                if type(value) == dict and 'slug' in value and 'pretty' in value:\n",
    "                    rectified_entry[key]  = value['pretty']\n",
    "                    rectified_entry[key + '_norm'] = value['slug']\n",
    "                else:\n",
    "                    rectified_entry[key] = value\n",
    "            with open('output/the_numbers_rectified.csv', 'a', newline='') as outfile:\n",
    "                row_writer = csv.DictWriter(outfile, columns)\n",
    "                row_writer.writerow(rectified_entry)\n",
    "            if rectified_entry['release_date_norm'] is not None:\n",
    "                result = re.search('^(\\d{4}).*', rectified_entry['release_date_norm'])\n",
    "                year = result.group(1) if result is not None else None\n",
    "            log = '{:0>4} {}'.format(i, rectified_entry['title'])\n",
    "            if year is not None:\n",
    "                log += ' ({})'.format(year)\n",
    "            print(log)\n",
    "            i += 1\n",
    "    \n",
    "    def rectify_lst(df, imdb_id, lst):\n",
    "        return pd.concat([pd.DataFrame([imdb_id, el], columns=['imdb']) for el in lst])\n",
    "    \n",
    "    def rectify_lsts(the_numbers, output_dir):\n",
    "        lst_columns = ['franchises', 'keywords', 'production_companies', 'production_countries', 'languages']\n",
    "        pretty_columns = [(col, 'pretty', col) for col in lst_columns]\n",
    "        slug_columns = [(col, 'slug', col + '_norm') for col in lst_columns]\n",
    "        all_columns = pretty_columns + slug_columns\n",
    "        dataframe_dict = {dest_col: pd.DataFrame([], columns=['imdb_id', dest_col]) \n",
    "                          for source_col, specifier, dest_col in all_columns}\n",
    "        for entry in the_numbers:\n",
    "            if 'imdb_id' not in entry:\n",
    "                continue\n",
    "            imdb_id = entry['imdb_id'] \n",
    "            for source_col, specifier, dest_col in all_columns:\n",
    "                if source_col in entry:\n",
    "                    # print(source_col, ' ', specifier, ' ', dest_col)\n",
    "                    # dataframe_dict[dest_col] = dataframe_dict[dest_col].append(pd.concat([pd.DataFrame([imdb_id, el], columns=dataframe_dict[dest_col].columns) for el in entry[source_col][specifier]]))\n",
    "                    dataframe_dict[dest_col].append(pd.concat([pd.DataFrame([[imdb_id, el[specifier]]], columns=dataframe_dict[dest_col].columns) for el in entry[source_col]]))\n",
    "        display(dataframe_dict['franchises'])\n",
    "        \n",
    "    def rectify():\n",
    "        with open('output/the_numbers_norm.json', 'r') as infile:\n",
    "            the_numbers = json.load(infile)\n",
    "        output_dir = os.path.join('output', 'rectified')\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        # rectify_single(the_numbers, output_dir)\n",
    "        rectify_lsts(the_numbers, output_dir)\n",
    "        \n",
    "    rectify()\n",
    "    \n",
    "rectify_the_numbers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data():\n",
    "    import pandas as pd\n",
    "    \n",
    "    df_movie_ratings = pd.read_csv('output/norm_movie_ratings.csv', encoding = \"ISO-8859-1\")\n",
    "    df_the_numbers = pd.read_csv('output/the_numbers_rectified.csv', encoding = \"ISO-8859-1\")\n",
    "    display(df_movie_ratings.head())\n",
    "    display(df_the_numbers.head())\n",
    "    df_merged = pd.merge(df_movie_ratings, df_the_numbers, left_on='imdb_ids', right_on='imdb_id')\n",
    "    df_merged.to_csv('output/merged.csv', index=False)\n",
    "    \n",
    "merge_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
